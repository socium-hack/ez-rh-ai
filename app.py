# import streamlit as st
# import os
# import tempfile
# from langchain_community.document_loaders import PDFPlumberLoader
# from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_core.vectorstores import InMemoryVectorStore
# from langchain_ollama import OllamaEmbeddings
# from langchain_ollama.llms import OllamaLLM
# from langchain_core.prompts import ChatPromptTemplate

# # --------------------------------------------------------------------
# # CONFIGURATION GENERALE STREAMLIT
# # --------------------------------------------------------------------
# st.set_page_config(
#     page_title="DeepSeek HR Solutions",
#     page_icon="üìÑ",
#     layout="wide"
# )

# # --------------------------------------------------------------------
# # INITIALISATION DU MODELE & EMBEDDINGS (SANS CACHE)
# # --------------------------------------------------------------------
# # Attention : Sur un vrai syst√®me, recharger le mod√®le √† chaque refresh peut √™tre co√ªteux
# embeddings = OllamaEmbeddings(model="deepseek-r1:7b")
# model = OllamaLLM(model="deepseek-r1:7b")

# # Cr√©e un vector store vide (utilis√© √©ventuellement pour la partie RAG)
# global_vector_store = InMemoryVectorStore(embeddings)

# # --------------------------------------------------------------------
# # FONCTIONS UTILES : TELECHARGEMENT, LECTURE & SPLIT PDF
# # --------------------------------------------------------------------
# def save_uploaded_file(uploaded_file):
#     """
#     Sauvegarde un fichier upload√© en local (temporaire)
#     et retourne le chemin absolu.
#     """
#     temp_dir = "temp"
#     os.makedirs(temp_dir, exist_ok=True)
#     temp_path = os.path.join(temp_dir, uploaded_file.name)
#     with open(temp_path, "wb") as f:
#         f.write(uploaded_file.getbuffer())
#     return temp_path

# def load_pdf(file_path):
#     """
#     Charge un PDF avec PDFPlumberLoader et renvoie une liste de Documents.
#     """
#     loader = PDFPlumberLoader(file_path)
#     return loader.load()

# def split_text(documents, chunk_size=1000, chunk_overlap=200):
#     """
#     Segmente le texte en chunks avec RecursiveCharacterTextSplitter.
#     """
#     text_splitter = RecursiveCharacterTextSplitter(
#         chunk_size=chunk_size,
#         chunk_overlap=chunk_overlap,
#         add_start_index=True
#     )
#     return text_splitter.split_documents(documents)

# # --------------------------------------------------------------------
# # 1) MATCHING CV / OFFRE
# # --------------------------------------------------------------------
# comparison_template = """
# You are an expert in resume analysis. Compare the provided resume with the job description.
# Evaluate the match based on skills, experience, and keywords.
# Do not include or reveal your internal reasoning or any <think> section.


# Resume:
# {resume}

# Job Description:
# {job_description}

# Provide a compatibility score (out of 100) and highlight strengths and weaknesses.
# Answer:
# """

# def analyze_match(resume_text, job_text):
#     """
#     G√©n√®re un score de compatibilit√© + feedback sur les forces/faiblesses.
#     """
#     prompt = ChatPromptTemplate.from_template(comparison_template)
#     chain = prompt | model
#     return chain.invoke({"resume": resume_text, "job_description": job_text})

# # --------------------------------------------------------------------
# # 2) Q&A (RAG)
# # --------------------------------------------------------------------
# qa_template = """
# You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.
# If you don't know the answer, just say that you don't know.
# GIVE ALL THE IMPORTANT DETAILS AND keep the answer concise.
# REPONDS TOUJOURS EN FRANCAIS SAUF SI LA QUESTION EST POSEE EN ANGLAIS 
# Do not include or reveal your internal reasoning or any <think> section.


# Question: {question}
# Context: {context}
# Answer:
# """

# def retrieve_docs(query, vector_store, k=3):
#     """
#     R√©cup√®re les k documents les plus pertinents depuis un vector_store donn√©.
#     """
#     return vector_store.similarity_search(query, k=k)

# def answer_question(question, documents):
#     """
#     Construit le prompt, interroge le LLM et renvoie une r√©ponse concise.
#     """
#     context = "\n\n".join([doc.page_content for doc in documents])
#     prompt = ChatPromptTemplate.from_template(qa_template)
#     chain = prompt | model
#     return chain.invoke({"question": question, "context": context})

# # --------------------------------------------------------------------
# # INTERFACE STREAMLIT
# # --------------------------------------------------------------------
# st.title("üîç DeepSeek HR Solutions")

# tabs = st.tabs(["1Ô∏è‚É£ Matching CV - Offre", "2Ô∏è‚É£ Q&A (RAG) sur PDF"])

# # --------------------------------------------------------------------
# #  ONGLET 1 : MATCHING CV & OFFRE
# # --------------------------------------------------------------------
# with tabs[0]:
#     st.header("Analyse de compatibilit√© entre CV et Offre d'emploi")

#     col1, col2 = st.columns(2)
#     with col1:
#         uploaded_resume = st.file_uploader("üìÑ Upload ton CV (PDF uniquement)", type=["pdf"])
#     with col2:
#         uploaded_job = st.file_uploader("üìë Upload l'offre d'emploi (PDF ou TXT)", type=["pdf", "txt"])

#     if uploaded_resume and uploaded_job:
#         st.subheader("üìä R√©sultats de l'analyse")

#         # Sauvegarde des fichiers
#         resume_path = save_uploaded_file(uploaded_resume)
        
#         # Traitement CV
#         with st.spinner("üìù Lecture du CV..."):
#             resume_docs = load_pdf(resume_path)
#             resume_chunks = split_text(resume_docs)
#             resume_text = "\n".join([doc.page_content for doc in resume_chunks])

#         # Traitement Job Description
#         job_text = ""
#         if uploaded_job.type == "application/pdf":
#             job_path = save_uploaded_file(uploaded_job)
#             with st.spinner("üìù Lecture de l'offre..."):
#                 job_docs = load_pdf(job_path)
#                 job_chunks = split_text(job_docs)
#                 job_text = "\n".join([doc.page_content for doc in job_chunks])
#         else:
#             # Si c'est un fichier texte pur
#             job_text = uploaded_job.getvalue().decode("utf-8")

#         # Analyse
#         with st.spinner("üìä Analyse en cours..."):
#             result = analyze_match(resume_text, job_text)

#         st.success("‚úÖ Analyse termin√©e !")
#         st.write(result)
#     else:
#         st.info("Veuillez uploader un CV et une offre d'emploi pour commencer l'analyse.")

# # --------------------------------------------------------------------
# #  ONGLET 2 : Q&A (RAG) sur PDF
# # --------------------------------------------------------------------
# with tabs[1]:
#     st.header("Recherche et Question-R√©ponse sur PDF")

#     # Choix du PDF √† indexer
#     rag_pdf = st.file_uploader("Upload un PDF pour indexation (RAG)", type=["pdf"])
    
#     if rag_pdf:
#         with st.spinner("üîÑ Indexation du PDF..."):
#             # Stockage temporaire
#             with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
#                 temp_file.write(rag_pdf.getbuffer())
#                 temp_pdf_path = temp_file.name
            
#             # On cr√©e un NOUVEAU vector store propre √† ce PDF
#             local_vector_store = InMemoryVectorStore(embeddings)
            
#             # Lecture et segmentation
#             documents = load_pdf(temp_pdf_path)
#             chunked_docs = split_text(documents)
#             local_vector_store.add_documents(chunked_docs)
        
#         st.success(f"‚úÖ PDF {rag_pdf.name} index√© avec succ√®s !")

#         # Champ pour poser la question
#         question = st.text_input("Pose ta question au document :")
#         if question:
#             st.write("**Question** :", question)
#             with st.spinner("üí¨ Recherche..."):
#                 related_docs = retrieve_docs(question, local_vector_store, k=3)
#                 answer = answer_question(question, related_docs)
#             st.write("**R√©ponse** :")
#             st.write(answer)
#     else:
#         st.info("Upload un PDF pour activer la fonctionnalit√© Q&A.")
# Home.py

import streamlit as st

st.set_page_config(
    page_title="DeepSeek HR Solutions",
    page_icon="üìÑ",
    layout="wide"
)

def main():
    st.title("Bienvenue sur DeepSeek HR Solutions")
    st.subheader("Un hub local pour analyser des CV, faire du Matching et effectuer du Q&A sur vos documents PDF.")
    
    st.markdown("""
    ### Description du Projet
    
    **DeepSeek HR Solutions** est un outil de d√©monstration montrant comment on peut utiliser des mod√®les de langage (LLM) et des embeddings
    pour plusieurs cas d'usage en **recrutement** et en **traitement documentaire** :
    
    - **Matching CV ‚Äì Offre d‚Äôemploi** :  
      T√©l√©versez un CV et une offre, puis obtenez un score de compatibilit√© et des retours sur les forces/faiblesses.

    - **Recherche et Question-R√©ponse (RAG) sur PDF** :  
      Indexez un PDF en local, posez-lui des questions en langage naturel et recevez des r√©ponses cibl√©es.

    ### Navigation
    Pour acc√©der aux fonctionnalit√©s, utilisez le menu `Pages` sur la gauche :
    - [1Ô∏è‚É£ CV ‚Äì Offre Matching](./pages/1_CV_Offre_Matching.py)
    - [2Ô∏è‚É£ Q&A (RAG) sur PDF](./pages/2_QA_RAG.py)

    ---
    ### Tech Stack
    - **Streamlit** pour l'interface web
    - **LangChain** & **Ollama** pour l‚ÄôIA locale (mod√®le DeepSeek)
    - **PDFPlumber** pour l‚Äôextraction de texte PDF
    - **InMemoryVectorStore** pour l‚Äôindexation locale des embeddings
    """)
    
    st.info("Naviguez dans le menu √† gauche pour d√©couvrir les fonctionnalit√©s.")

if __name__ == "__main__":
    main()
